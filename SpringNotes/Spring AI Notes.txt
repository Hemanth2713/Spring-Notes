## Spring AI Introduction

Spring Boot is a powerful framework for building enterprise applications, offering modules like Spring Web, Spring JPA, and Spring Security to create secure and robust solutions. However, in today’s AI-driven world, integrating artificial intelligence into these applications has become essential. Whether it’s drafting essays, composing emails, or answering user queries, tools like ChatGPT or Gemini are commonly used by inputting prompts to generate responses. These responses are often copied, slightly modified, and utilized. The challenge arises when you want to incorporate AI features into enterprise applications, such as an e-commerce platform or any application requiring functionalities like chatbots, automated product descriptions, or image generation. While one approach could involve building a Java-based application with Spring and integrating AI solutions developed in Python, this requires managing API calls between two languages and coordinating multiple teams. Spring AI simplifies this by providing a Java-based layer to interact with large language models (LLMs), eliminating the complexity of cross-language integrations. LLMs, such as those from OpenAI, Anthropic, or locally run models via Ollama, are often proprietary and offered as services. By creating an account, adding credits, and obtaining an API key, developers can send requests to these models and receive responses. Spring AI acts as an intermediary layer, enabling seamless integration of generative AI features into enterprise applications. It supports various model types, including chat completion, text embedding, text-to-image, audio transcription, text-to-speech, and content moderation, making it easier to enhance applications with AI capabilities.

## Why Spring AI

Spring AI is crucial for injecting generative AI features into enterprise applications, acting as a bridge between the application and various LLM providers like OpenAI, Anthropic, Azure OpenAI, and others. While it’s possible to directly connect an application to a provider like OpenAI using their SDKs, this approach has limitations. For instance, OpenAI provides a Java SDK that allows developers to add dependencies, set an API key, and send prompts to their models. Similarly, Anthropic offers SDKs, though they may differ in implementation and could still be in beta. Directly integrating with a single provider’s SDK works if you’re committed to one model, but it becomes problematic when you need to support multiple models or switch providers, as each has unique code requirements. This leads to significant code refactoring if a provider change is needed. Spring AI addresses this by providing an abstraction layer, allowing developers to write model-agnostic code. It supports major AI providers like OpenAI, Anthropic, Microsoft, Amazon, Google, and Ollama, enabling seamless interaction with multiple models without altering the core application logic. This abstraction ensures cleaner, more manageable code, which is critical for enterprises aiming to maintain flexibility and scalability in their AI integrations.

## Spring AI Documentation

The Spring AI documentation, accessible via the Spring projects page under the “Learn” section, offers comprehensive guidance for developers. Currently in SNAPSHOT, with a general availability (GA) version expected soon, the documentation outlines how Spring AI streamlines AI application development by reducing complexity. Inspired by LangChain, Spring AI isn’t a direct replacement but provides similar functionalities within a Java ecosystem. It emphasizes abstractions that simplify interactions with LLMs, eliminating the need to write complex, model-specific code. The documentation covers concepts like Retrieval-Augmented Generation (RAG), chat clients, and various chat models tailored to providers like OpenAI, Anthropic, Azure OpenAI, DeepSeek, Vertex AI, and Ollama, among others. As the list of supported models grows, the documentation is regularly updated to reflect new capabilities. Developers are encouraged to refer to it for detailed instructions and to explore features like chat models and advisors, which enhance application functionality.

## Creating a Spring AI Project

To create a Spring AI project, use Spring Initializr (start.spring.io) to generate a Maven-based Spring Boot project with Java as the language. Specify a group ID (e.g., com.telusko) and artifact ID (e.g., SpringAICode), and add dependencies for Spring Web and Spring AI’s OpenAI module. Additional dependencies for other models, like Anthropic or Ollama, can be included based on requirements. After generating and downloading the project, open it in an IDE like IntelliJ. The POM file will list the added dependencies, such as spring-boot-starter-web and spring-ai-openai-spring-boot-starter. Running the project initially may fail due to a missing OpenAI API key, which must be set in the application.properties file as `spring.ai.openai.api-key`. This key is essential for authenticating requests to the OpenAI service.

## Creating an OpenAI Key

To obtain an OpenAI API key, visit openai.com, log in, and navigate to the API platform. In the settings section, go to “Billing” to ensure sufficient credits are available, as OpenAI’s services are not free and charge based on token usage. Add the minimum required balance and disable auto-recharge to avoid unexpected charges. Then, in the “API Keys” section, create a new secret key, naming it (e.g., SpringAI-Demo) and assigning it to the default project with full permissions. Copy and securely store the key, as it’s displayed only once. Paste this key into the application.properties file to enable OpenAI integration. Running the project again should succeed, though AI features require additional coding to function.

## Asking Questions to the OpenAI Model

With the project set up and the OpenAI API key configured, test AI functionality using an API client like Insomnia. Send a GET request to `localhost:8080/api/{message}`, where `{message}` is the prompt (e.g., “What is Java?” or “Tell me a tech joke”). Initially, a 404 error occurs because no controller handles the request. Create an `OpenAIController` class annotated with `@RestController`, and define a `getAnswer` method with `@GetMapping("/api/{message}")` to capture the prompt via `@PathVariable`. Inject a `ChatModel` (specifically `OpenAIChatModel`) using constructor-based dependency injection. Use the `chatModel.call(message)` method to send the prompt to OpenAI and retrieve the response. Return this response wrapped in a `ResponseEntity.ok()`. Testing with Insomnia should now yield responses, such as explanations or jokes, demonstrating successful communication with the OpenAI server.

## Working with ChatClient

While `ChatModel` enables basic LLM interactions, `ChatClient` offers a more flexible and fluent API for advanced use cases. Built on top of `ChatModel`, `ChatClient` is created programmatically using `ChatClient.create(chatModel)`. Replace `ChatModel` with `ChatClient` in the controller, and update the method to use `chatClient.prompt(message).call().content()` to fetch the response. This approach provides greater control, allowing developers to manipulate prompts, access metadata, or handle streaming responses. Testing with Insomnia confirms that `ChatClient` works similarly but offers additional flexibility for complex interactions, such as structured outputs or multimodal data (e.g., images, audio).

## ChatResponse and Metadata

Using `ChatClient`, developers can access more than just the response text by leveraging the `ChatResponse` object. Instead of calling `.content()`, use `.chatResponse()` to obtain a `ChatResponse` object, which contains the response text and metadata. Extract the text using `chatResponse.getResult().getOutput().getText()`. Access metadata via `chatResponse.getMetadata()`, which provides details like the model used (e.g., GPT-4o mini), rate limits, or usage statistics. Log metadata to the console for debugging or monitoring. This approach allows developers to handle multiple responses (e.g., a list of jokes) and perform validation, such as checking for null responses, enhancing application robustness.

## ChatClient Builder

Spring AI’s `ChatClient.Builder` offers an alternative to programmatically creating `ChatClient` instances. Autoconfigured by Spring Boot, the builder automatically detects the model (e.g., OpenAI) when only one is configured. Inject `ChatClient.Builder` into the controller and create a `ChatClient` using `builder.build()`. This simplifies configuration for single-model applications. However, with multiple models (e.g., OpenAI and Ollama), the builder may cause conflicts, requiring manual configuration or disabling autoconfiguration. Testing with Insomnia confirms that the builder works seamlessly for single-model setups, reducing boilerplate code.

## Spring AI Memory Advisor

By default, LLMs are stateless and don’t retain conversation history. Spring AI addresses this with advisors, which intercept and modify requests or responses. To enable memory, use `MessageChatMemoryAdvisor` with `InMemoryChatMemory`. Configure the advisor in the `ChatClient` creation process by adding `.defaultAdvisors(new MessageChatMemoryAdvisor(new InMemoryChatMemory()))` before `.build()`. This stores chat history in memory, allowing the application to recall prior messages. Testing with Insomnia (e.g., “Tell me a tech joke” followed by “more”) shows that the advisor enables contextual responses, mimicking the behavior of tools like ChatGPT. Advisors also support other use cases, such as content filtering.

## Running Model Locally with Ollama

Running LLMs locally avoids cloud service costs and enables customization. Ollama facilitates this by allowing models like Llama, DeepSeek, Mistral, or Gemma to run on local machines. Download Ollama from its website, install it, and verify its presence using the `ollama` command in a terminal. List available models with `ollama list` or download a model (e.g., Mistral) using `ollama run mistral`. Test the model locally by sending prompts like “What is Spring AI?” or “Tell me a joke.” Local models may require training for up-to-date data but support conversational memory by default. Hardware requirements vary, with larger models (e.g., DeepSeek 32b) needing significant RAM and GPU power.

## Spring AI with Ollama

To integrate Ollama with Spring AI, add the Ollama dependency (`spring-ai-ollama-spring-boot-starter`) to the POM file and reload Maven. Create an `OllamaController` similar to `OpenAIController`, but use `OllamaChatModel` instead. Comment out conflicting OpenAI configurations to avoid injection issues. By default, Spring AI uses Mistral if available; otherwise, specify the model (e.g., DeepSeek) in application.properties using `spring.ai.ollama.chat.options.model=deepseek:7b`. No API key is required for local execution. Test with Insomnia, sending prompts like “Tell me a joke,” and verify responses. If the specified model is missing, an error occurs, which can be resolved by downloading the model via Ollama or updating the configuration. This setup enables cost-free, local AI integration with Spring AI.